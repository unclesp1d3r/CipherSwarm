# Nginx reverse proxy configuration for CipherSwarm production load balancing.
#
# REASONING:
#   Why: A single web instance becomes a bottleneck when many cracking agents
#        send concurrent status updates. Nginx distributes requests across
#        multiple Thruster/Puma replicas using Docker DNS service discovery.
#   Alternatives considered: HAProxy (heavier), Traefik (more complex config),
#        Docker Swarm ingress (requires Swarm mode).
#   Decision: Nginx is lightweight, well-understood, and works with plain
#        Docker Compose scaling via the embedded DNS resolver.
#   Performance: least_conn algorithm ensures even distribution under load;
#        keepalive connections reduce TCP overhead between nginx and backends.
#   Future: Consider Traefik if automatic TLS and container-aware routing
#        become requirements. Evaluate nginx Unit for application-level
#        integration.

worker_processes auto;

error_log /dev/stderr notice;
pid       /tmp/nginx.pid;

events {
    worker_connections 1024;
}

http {
    # Docker embedded DNS resolver for dynamic upstream resolution.
    resolver 127.0.0.11 valid=10s ipv6=off;

    # Custom log format with upstream context for diagnosing load distribution.
    log_format upstream_log '$remote_addr - $remote_user [$time_local] '
                            '"$request" $status $body_bytes_sent '
                            '"$http_referer" "$http_user_agent" '
                            'upstream=$upstream_addr '
                            'upstream_status=$upstream_status '
                            'upstream_response_time=$upstream_response_time '
                            'request_time=$request_time';

    access_log /dev/stdout upstream_log;

    # Upstream pool targeting the scaled 'web' service.
    # Docker DNS resolves 'web' to all replica container IPs.
    upstream web_backend {
        # Shared memory zone required for the 'resolve' parameter.
        zone web_backend_zone 64k;
        least_conn;

        # 'resolve' re-queries DNS so new/removed replicas are picked up
        # automatically. Requires nginx >= 1.27.3 (open-sourced from nginx Plus).
        # max_fails / fail_timeout control passive health checks.
        server web:80 resolve max_fails=3 fail_timeout=30s;

        # Keep persistent connections to backends for efficiency.
        keepalive 32;
    }

    # Dedicated health endpoint for Docker healthcheck (avoids depending on backends).
    server {
        listen 8080;

        location /nginx-health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }
    }

    server {
        listen 80;

        # Allow large file uploads (hash lists, word lists, rule files).
        client_max_body_size 100M;

        # WebSocket support for Action Cable (Turbo Streams via Solid Cable).
        location /cable {
            proxy_pass http://web_backend;
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_set_header X-Forwarded-Host $host;
            proxy_read_timeout 86400s;
        }

        location / {
            proxy_pass http://web_backend;

            # Use HTTP/1.1 with empty Connection header for keepalive.
            proxy_http_version 1.1;
            proxy_set_header Connection "";

            # Preserve client information for Rails request handling.
            proxy_set_header Host              $host;
            proxy_set_header X-Real-IP         $remote_addr;
            proxy_set_header X-Forwarded-For   $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_set_header X-Forwarded-Host  $host;

            # Timeouts â€” read is longer for potentially slow API responses.
            proxy_connect_timeout 60s;
            proxy_send_timeout    60s;
            proxy_read_timeout    300s;

            # Retry the next upstream on transient errors (GET/HEAD only).
            # nginx does NOT retry non-idempotent methods (POST/PATCH/PUT/DELETE)
            # by default, so agent API write operations are safe from duplication.
            proxy_next_upstream error timeout http_502 http_503 http_504;
            proxy_next_upstream_tries 3;
            proxy_next_upstream_timeout 30s;
        }
    }
}
